# Faster R-CNN #

- 핵심 아이디어
    
    Fast R-CNN의 단점은 selective search를 수행하는 region proposal 부분이 외부에 존재하기 때문에 inference에서 bottleneck을 일으킨다는 것이었다.

    따라서 Faster R-CNN의 핵심 아이디어는 Region Proposal Network(RPN) 자체를 학습한다는 것이다. 

    기존 Fast R-CNN의 구조를 그대로 가져오지만 selective search를 제거하고 RPN을 통해 RoI를 계산한다.
    
    이를 통해 GPU를 통한 RoI 계산이 가능해졌고 RoI계산을 학습시켜 정확도를 높일 수 있었다.
    
    RPN은 selective search가 2000개의 RoI를 계산하는데 반해 800개 정도의 RoI를 계산하면서도 더 높은 정확도를 보인다. 
   
- Faster R-CNN의 전반적인 구조

![image](https://user-images.githubusercontent.com/66320010/107392751-564d0f80-6b3d-11eb-8a31-2837d4eb5cfe.png)

   구조를 살펴보면 Feature Map을 먼저 추출한 다음 이를 RPN에 전달하여 RoI를 계산한다.

   여기서 얻은 RoI로 RoI Pooling을 진행하고 classfier를 진행하여 object detection을 수행한다.
  
# Region Proposal Network(RPN) #

![image](https://user-images.githubusercontent.com/66320010/107396599-40d9e480-6b41-11eb-8410-f222362bf673.png)

위 그림은 RPN의 개념을 시각적으로 보여주지만 이해가 어려울 수 있어 더 풀어서 그림으로 표현하면 다음과 같다.

![image](https://user-images.githubusercontent.com/66320010/107397104-b34ac480-6b41-11eb-9529-9c19c602b0cc.png)

- RPN 동작 알고리즘 

  1) CNN을 통해 뽑아낸 feature map을 입력으로 받는다(이 때, feature map의 크기를 H * W* C로 가정한다)
  
  2) Feature map에 3 * 3 컨볼루션을 256 또는 512 채널만큼 수행한다(위의 그림에서 intermediate layer에 해당). 이 때 padding을 1로 설정하여 H * W가 보존될 수 있도록 한다 -> 수행결과 H * W * 256 또는 H * W * 512 크기의 두 번째 feature map을 얻게된다.

  3) 두 번째 feature map을 입력받아 classification과 bounding box regression 예측값을 계산한다. 이 때 fully connected layer가 아니라 1 * 1 컨볼루션 이용하여 계산하는 fully convolution network 특징을 갖는다.
  
  4) Classification을 수행하기 위해 1 * 1 컨볼루션을 채널 수 만큼 수행해주며 그 결과로 H * W * 18크기의 feautre map을 얻는다( 여기서 18은 2(오브젝트인지 나타내는 지표 수) * 9(앵커 박스 갯수)를 의미 ) -> 이 값들을 적절히 reshape해주고 softmax를 적용하여 해당 앵커박스가 오브젝트일 확률 값을 얻는다
  
  5) Bounding box regression 예측 값을 얻기 위해 1 * 1 컨볼루션을 채널 수(4 * 9)만큼 수행한다.
  
  6) 이제 앞서 얻은 값들로 RoI를 계산한다. 먼저 classification을 통해 얻은 오브젝트일 확률 값들을 정렬하고 높은 순으로 K개의 앵커박스만 추려낸다. 그리고 난 후 K개의 앵커박스들에 각각 bounding box regression을 적용해준다. 그 다음 NMS(Non Maximum Suppression)을 적용해 RoI를 구한다.
  
  **이렇게 찾은 RoI를 첫 번째 feature map(Pretrained CNN거치고 난 후의 feature map)에 projection한 다음 RoI Pooling적용하고 classification에 적용한다.**
  
- RPN 동작 예시

1) Anchor targeting
    
![image](https://user-images.githubusercontent.com/66320010/107400960-b8aa0e00-6b45-11eb-9a02-30e143a0ad02.png)
    
shape이 800 * 800 * 3인 input image가 CNN(VGG-16)을 거치면 50 * 50 * 512 feature map이 생성된다.
    
이 feature map에서 각 중심을 기준으로 9개의 anchor box를 만든다. 

-> anchor box는 3개의 scale과 3개의 ratio사용해 총 9개가 만들어 지는 것이다.

![image](https://user-images.githubusercontent.com/66320010/107411536-fc0a7980-6b51-11eb-9ed2-f87bfa457502.png)

즉, 800 * 800 에서 50 * 50의 feature map이 생성된다고 했을 때 anchor box는 50 * 50 의 각 중심을 기준으로 9개를 만들어서 총 50 * 50 * 9 = 22500 개를 만드는 것이다.
    
이렇게 만들어낸 22500개의 anchor box들을 가지고 그 안에 오브젝트(물체)가 있는지 없는지 학습을 할 것이다.

그러기 위해서는 우선 이미지와 그에 대한 ground truth box가 들어왔을 때 각 anchor box마다 이 박스가 물체를 감싸고 있는지 배경을 감싸고 있는지 labeling을 해줘야 한다.

GT Label은 만들어진 22500개의 anchor box들과 ground truth box의 IoU를 모두 계산하여 IoU가 0.7보다 클 경우 1, 0.3보다 작을 경우 0으로 두고 나머지는 -1로 둔다.

이 과정을 통해 Ground truth box마다 IoU가 가장 높은 anchor box 1개를 뽑아 1로 labeling한다.

![image](https://user-images.githubusercontent.com/66320010/107404115-45a29680-6b49-11eb-9574-7c451d6c9f75.png)

![image](https://user-images.githubusercontent.com/66320010/107404370-90bca980-6b49-11eb-8931-b6a1cfc498ed.png)

*IoU(Intersection over Union)란 두 개의 bounding box가 겹치는 비율을 의미하며 두 영역의 교차 영역 넓이를 합 영역의 넓이 값으로 나눈 값이다.*

2) Prediction

![image](https://user-images.githubusercontent.com/66320010/107405178-63243000-6b4a-11eb-99fd-189d0ee2c5a6.png)

![image](https://user-images.githubusercontent.com/66320010/107409430-81d8f580-6b4f-11eb-9113-579c31a28dc8.png)

   - bbox regression layer(bounding box위치 predict)
    
   800 * 800 * 3의 input image를 CNN을 통해 50 * 50 * 512 크기의 feature map을 만들고 이를 RPN Network에 넣어준다.
   
   그 후 1 * 1 conv를 채널 수(36)만큼 진행하면 50 * 50 * 36 크기의 result가 나온다.
   
   ![image](https://user-images.githubusercontent.com/66320010/107406705-117ca500-6b4c-11eb-9138-b98f3d581290.png)
   
   -> 이 때, 36은 anchor box 9개 * bounding box 좌표 4개를 의미한다.
    
   - classification layer(물체 존재 여부 predict)
   
   위와 동일하게 800 * 800 * 3의 input image를 CNN을 통해 50 * 50 * 512 크기의 feature map을 만들고 이를 RPN Network에 넣어준다.
   
   그 후 1 * 1 conv를 채널 수(18)만큼 진행하면 50 * 50 * 18 크기의 result가 나온다.
   
   -> 이 때, 18은 anchor box 9개 * class갯수 2개를 의미한다.
   
   ![image](https://user-images.githubusercontent.com/66320010/107409502-9ae1a680-6b4f-11eb-8a49-ba0de909a24e.png)
   
**이렇게 prediction한 값으로 1) Anchor targeting에서 구했던 이미지의 ground truth label과 함께 loss function을 통해 RPN을 학습하게 된다.**

**그와 동시에 prediction된 값(class,bbox regression)은 NMS를 거쳐 특정 갯수의 RoI로 샘플링 된다.**

# Loss Fucntion #

RPN을 학습시키기 위한 Loss에 대해 알아본다.

![image](https://user-images.githubusercontent.com/66320010/107479791-f139fe00-6bbe-11eb-9348-338bd64727f1.png)

RPN은 Classification과 Bounding Box Regression을 수행하는데 loss function은 이 두가지에서 얻은 loss를 엮은 형태이다.















