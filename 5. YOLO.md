# YOLO(You Only Look Once) #

- YOLO란?

  - YOLO는 You Only Look Once, 즉 이미지를 한 번 보는 것만으로 물체의 종류와 위치를 추측하는 딥러닝 기반의 물체인식 알고리즘을 뜻한다.

  - YOLO는 2015년에 나온 논문으로 Faster R-CNN에 비하여 부려 6배 가량 빠른 속도를 보인다. 정확도는 비록 조금 낮다 하더라고 정말 비약적인 발전이라고 할 수 있다.
  
  - **주요 기여 : 1 Step Object Detection 기법을 제시하였으며, fps가 무려 45로 속도 측면에서 획기적인 발전을 이루었다.**
  
 ![image](https://user-images.githubusercontent.com/66320010/107498757-8648f100-6bd7-11eb-893f-3743f809d67e.png)

  기존 R-CNN 계열의 검출 네트워크들은 이미지 안에서 bounding box를 생성하기 위해 region proposal이라는 방법을 사용한다.
  
  그렇게 제안된 bounding box에 classifier를 적용하여 분류(classification)한 뒤 bounding box를 조정하고, 중복된 검출을 제거하고, 객체에 따라 box의 점수를 재산정하기 위해 후처리(post-processing)를 한다.
  
  **=> 이런 복잡함 때문에 R-CNN은 속도가 느려지며 각 절차를 독립적으로 훈련시켜야 하므로 최적화(optimization)하기도 힘들다.**
  
  따라서 저자들은 객체 검출을 하나의 회귀 문제로 보고 절차를 개선하였다. 이미지의 픽셀로 부터 bounding box의 위치(coordinates), 클래스 확률(class probabilities)을 구하기까지의 일련을 절차를 하나의 회귀 문제로 재정의하였다.
  
  이러한 시스템을 통해 YOLO는 이미지 내에 어떤 물체가 있고 그 물체가 어디에 있는지를 하나의 파이프라인으로 구한다.
  
# Unified Detection #
  
YOLO가 기존의 object detection과 가장 크게 구분되는 부분은 기존에 1) region proposal 2) classification 이렇게 두 단계로 나누어 진행하던 방식에서 region proposal 단계를 제거하고 한번에 object detection을 수행하는 구조는 갖는다는 점이다.
  
![image](https://user-images.githubusercontent.com/66320010/107503026-cc548380-6bdc-11eb-9a18-e8c6af782c04.png)

위의 그림은 YOLO의 1 step 구조를 간단히 보여준다.

1) 먼저 입력 이미지를 S * S 그리드 영역으로 나눈다(실제 입력 이미지를 나누는 것은 아니다)

2) 각 그리드 영역에서 먼저 물체가 있을만한 영역에 해당하는 B개의 Bounding Box를 예측한다(이는 x,y,w,h로 나타내어 지는데 x,y는 bounding box의 중심점 좌표이며 w와h는 각각 너비와 높이이다)
  
3) 다음으로 해당 박스의 신뢰도를 나타내는 confidence를 계산한다(이는 해당 그리드에 물체가 있을 확률 Pr(Object)과 예측한 박스와 Ground Truth 박스와의 겹치는 영역을 비율을 나타내는 IoU를 곱해서 계산한 값을 의미한다)

![image](https://user-images.githubusercontent.com/66320010/107503730-a8457200-6bdd-11eb-9806-b06492e0c5cd.png)

4) 각각의 그리드마다 C개의 class에 대해 해당 클래스일 확률을 계산하며 수식은 아래 사진과 같다.

![image](https://user-images.githubusercontent.com/66320010/107503896-db880100-6bdd-11eb-8745-c656f8b0ffdc.png)

**이 때, 특이한 점은 기존의 Object Detection에서는 항상 클래스 수 + 1 (배경)을 집어넣어 classification을 하는데, yolo는 그렇지 않다.**
  
 # Network Design # 
 
 ![image](https://user-images.githubusercontent.com/66320010/107504335-77b20800-6bde-11eb-9c4c-345586f913bd.png)
 
 위의 그림은 YOLO의 전체 네트워크 구조를 보여준다. 
 
 YOLO는 24개의 Convolutional Layer(Conv Layer)와 2개의 Fully-Connected Layer(FC Layer)로 연결된 구조를 사용하고 있다.
  
  - Pre-trained Network
  
    주황색 테두리로 표현한 부분은 GoogLeNet을 이용하여 ImageNet 1000-class dataset을 사전에 학습한 결과를 Fine-Tuning한 네트워크를 말한다 (이 네트워크는 20개의 Conv Layer로 구성되어 있다)
  
    본 논문에서는 이 모델에서 88%의 정확도를 사전에 학습했다고 한다. 본래 ImageNet의 데이터 셋은 224x224의 크기를 가진 이미지 데이터이지만, 물체 인지를 학습할 때는 선명한 이미지 보다는 경계선이 흐릿한 이미지가 더 학습이 잘된다고 해서 Scale Factor를 2로 설정하여 이미지를 키워 448 x 448 x 3의 이미지를 입력 데이터로 받는다.
    
  - Reduction Layer
  
    보통 네트워크는 깊을 수록 더 많은 특징을 학습하기 때문에 정확도가 높아지는 경향이 있다. 하지만 Conv Layer를 통과할 때 사용하는 Filter 연산이 수행시간을 많이 잡아 먹기 때문에 무작정 네트워크를 깊게 쌓기에는 부담이 된다. 
  
    이 문제를 해결하기 위해 ResNet, GoogLeNet 등의 기법이 제안되었는데 오렌지 색 테두리로 된 영역에서는 GoogLeNet의 기법을 응용 하여 연산량은 감소하면서 층은 깊게 쌓는 방식을 이용하였다.
  
  - Training Network
  
    파란색 영역인 Training Network는 Pre-trained Network에서 학습한 feature를 이용하여 Class probability와 Bounding box를 학습하고 예측하는 네트워크이다.
    
    YOLO의 예측 모델은 S x S x (B * 5 + C) 개의 파라미터를 결과로 출력한다.
   
![image](https://user-images.githubusercontent.com/66320010/107505100-8f3dc080-6bdf-11eb-966a-90b4aeab733d.png)
 
해당 그림은 YOLO의 좀 더 직관적인 그림이다. 여기에서는 그리드 사이즈를 7, (B * 5 + C)를 30으로 가정한다.

네트워크의 출력은 7x7x30 feature map으로 그리드 별 bounding box와 confidence 지수, 그리고 각 클래스 별 예측 값들이 담겨져 있다.

![image](https://user-images.githubusercontent.com/66320010/107510059-c9f72700-6be6-11eb-830d-bd347b8e46bf.png)

 위의 그림을 보면 7x7 그리드 가운데 하나의 인덱스에 빨간 색 박스가 쳐져 있는 것을 볼 수 있으며 우리는 하나의 인덱스에서 B개의 Bounding box를 추측한다(논문에서는 이를 2로 설정하였다)
 
 하나의 박스는 중심점 x와 y, 너비와 높이 w,h 그리고 신뢰도 지수 C 이렇게 (x, y, w, h, C) 다섯개 차원의 벡터로 나타낼 수 있으며, 두 개 박스는 10차원 벡터에 해당한다.
  
![image](https://user-images.githubusercontent.com/66320010/107507432-e6916000-6be2-11eb-8ead-184e0133ddf7.png)

두 개의 박스 정보 다음에 오는 20차원의 벡터는 해당 인덱스가 특정 클래스일 확률 값들이며 여기서는 클래스가 20개인 데이터 셋을 사용하였기 때문에 20차원 벡터로 표현된다.

앞에서 박스의 신뢰도를 Pr(obj) * IoU로 구했고, 각 클래스별 확률 값을 구할 때는 Pr(classi | object) 로 구했다. 

이 둘을 곱해주면 Pr(classi) * IoU 가 되고, 이는 곧 해당 박스가 특정 클래스일 확률 값이 된다. 

즉, 첫 번째 bbox의 confidence값과 class score을 곱하게 되면 첫 번째 bbox가 특정 클래스일 확률 값이 되는 것이다.

(bbox의 물체가 있을 확률인 confidence값이 매우 낮다면(0에 가깝다면) 그곳에 어떤 클래스가 있는지에 대한 정보도 매우 낮아지게 된다)

![image](https://user-images.githubusercontent.com/66320010/107510452-50ac0400-6be7-11eb-9584-5cf95fd3fc78.png)

이 작업을 인덱스 i의 모든 B개의 bounding box에 적용하고 이를 다시 SxS 인덱스에 적용하면 다음 사진과 같은 결과를 얻게된다.

![image](https://user-images.githubusercontent.com/66320010/107510583-83ee9300-6be7-11eb-8de7-7872366c20bf.png)

98개의 (특정)클래스 분류 점수 중 0.2(threshhold)보다 작은 것들은 0으로 채워준다. 0.2보다 작으면 경계 박스안에 뭐가 있는지는 모르겠지만 최소한 해당 클래스는 절대 아닐 것이라고 판단하는 것이다.

클래스 첫 번째 자리는 dog일 확률이다. dog일 확률을 높은 값 -> 낮은 값으로 정렬한다.

만약 위의 98개의 (특정)클래스 점수 표 중에서 97개가 0이고 하나만 0이 아니면 해당 오브젝트가 그 위치에 있다는 것을 간단하게 알 수 있다.

하지만 0이 아닌 박스가 여러 개 있을 수 있다. 따라서 이것들을 제거하기 위해서 NMS(비-최대값 억제)알고리즘을 사용하여 중복이 되는 bounding box들을 제거하여 오직 하나의 bounding box만 남긴다.

![image](https://user-images.githubusercontent.com/66320010/107511968-85b95600-6be9-11eb-829f-6f7170f644a1.png)

위의 그림은 dog에 대해 특정 클래스 확률 점수 표 98개 중 0.2보다 작은 값들은 0으로 만들고 난 후 0.2보다 값이 높은 경계 박스가 3개 남아있음을 나타낸다(0.1은 왜 들어가 있는 것인지 의문)

![image](https://user-images.githubusercontent.com/66320010/107511515-edbb6c80-6be8-11eb-9e3c-48d3b605c088.png)

dog에 대해 가장 높은 클래스 확률 점수를 갖는 경계 박스는 bbox47이었고 이를 오렌지 색으로 표시하였다.

![image](https://user-images.githubusercontent.com/66320010/107512096-b3060400-6be9-11eb-94ac-f678d847e7a8.png)

이 사진은 dog에 대해 두 번째로 값이 높은 경계 박스(초록색)에 대한 사진이다. 개를 어느정도 잘 표현했지만 오렌지색 경계 박스보단 나쁜 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/66320010/107512234-e183df00-6be9-11eb-900f-0059c13c5f89.png)

이 사진은 dog에 대해 세 번째로 값이 높은 경계 박스(파란색)에 대한 사진이다. 이 경계 박스는 개의 겨우 앞발만 포함하고 있음을 확인할 수 있다. 

![image](https://user-images.githubusercontent.com/66320010/107512618-6bcc4300-6bea-11eb-804c-f8d9e629ff10.png)

이러한 중복 경계 박스는 위에서 언급했듯이 NMS알고리즘을 통해 지우게 되는데 현재 값이 가장 높은 경계 박스는 0.5의 값을 가지고 있으며 두 번째는 0.3을 가지고 있다.

이 두 박스는 겹치는 부분이 50%가 넘는데 이런 경우 둘 중 하나의 경계박스는 중복일 것이라고 판단하여 둘 중 값이 더 낮은 것을 지워준다.

-> 따라서 0.3의 값을 0으로 바꿔준다.

![image](https://user-images.githubusercontent.com/66320010/107512875-c796cc00-6bea-11eb-807d-fdd963e88923.png)

dog에 대해 세 번째로 높은 경계 박스와 신뢰도가 가장 높은 오렌지색 경계 박스는 겹치는 부분이 50%가 넘지 않는다. 이 때는 파란색 경계 박스 안에 좌측의 개가 아닌 또 다른 개가 있을 수 있다고 판단한다. 

이런 경우는 값을 건드리지 않고 그냥 건너 뛴다.

지금까지 개에 대해 정렬한 첫번째 행에서는 경계 박스 2개(오렌지색 + 파란색)만 남고 나머지는 모두 값이 0이 되었다.

![image](https://user-images.githubusercontent.com/66320010/107513381-7804d000-6beb-11eb-9c6b-7c85aa03d1c1.png)

이제 남은 19개의 클래스(두번째 행부터 마지막 행까지)에 대해서도 같은 작업을 해준다.

하지만 우리는 사진에 개가 한마리인데 개에 대한 경계박스가 오렌지색, 파란색 2개가 남아있고 실제 파란색 박스에는 개가 없기 때문에 이를 지워주어야 한다.

bbox3은 20개의 클래스에 대한 값들을 가지고 있는데 이 중 가장 큰 값을 가지는 클래스가 바로 그 경계 박스의 클래스이다. 

이런 과정을 거쳐서 각 그리드 셀에는 최대값을 갖는 클래스 하나와 또 다른 클래스 하나, 총 2개가 나타날 수 있다. 

만약 한 그리드 셀에서 2개의 클래스가 검출되었다면 오브젝트들이 겹쳐있을 확률이 높을 것이다. 하나의 그리드 셀에서 클래스가 같은 2개의 오브젝트는 나올 수 없는 구조이다.

위의 예에서 개일 확률이 있는 2개의 경계 박스중에서 오렌지색만 진짜이고 파란색 경계박스는 잘못된 것이었다.

파란색 경계박스는 신뢰점수가 0.5 이하이므로 나중에 지워진다.

![image](https://user-images.githubusercontent.com/66320010/107513671-e6e22900-6beb-11eb-8a4a-7957a81aae9d.png)

앞서 말했듯이 98개의 경계 박스에 대해서 같은 작업을 하게되면 위와 같은 결과가 나오게 된다. 최종적으로 3개의 경계 박스만 나왔으며 경계 박스의 클래스는 컬러로 나타낸다.

# Loss Function #

YOLO는 loss function을 설계하기 전에 몇 가지 원칙을 만들었다.

1) 이미지를 분류하는 classifier 문제를 경계 박스를 만드는 regression(회귀)문제로 생각한다.

2) 경계 박스를 잘 그렸는지 평가하는 Localization Error와 박스 안의 물체를 잘 분류했는지 평가하는 Classification Error의 패널티를 다르게 평가한다. 특히, 박스 안의 물체가 없는 경우에는 Confidence Score를 0으로 만들기 위해 Localization Error 에 더 높은 패널티를 부과한다.

3) 많은 경계 박스중에 IoU 수치가 가장 높게 생성된 경계 박스만 학습에 참여한다. 이는 경계 박스를 잘 만드는 셀은 더욱 학습을 잘하도록 높은 Confidence Score를 주고 나머지 셀은 경계 박스를 잘 만들지 못하더라도 나중에 NMS 알고리즘을 통해 최적화 하기 위함이다.

![image](https://user-images.githubusercontent.com/66320010/107513965-5526eb80-6bec-11eb-9fa9-8eb91f28ded8.png)

- YOLO는 1번 원칙을 지키기 위해 Loss Fuction에서 Sum-Squared Error(SSD)를 사용한다.

- YOLO는 2번 원칙을 지키기 위해서 λ_coord와 λ_noobj 두 개의 변수를 사용한다.

- 위의 Loss Function에서 각 기호가 나타내는 의미는 다음과 같다.

    - S : 그리드 셀의 크기
    - B : 각 그리드 셀이 예측하는 경계 박스의 수
    - x, y, w, h : 바운딩 박스의 좌표 및 크기
    - 주황색 박스(1번)5로 설정된 λ_coord 변수로서 Localization error에 5배 더 높은 패널티를 부여하기 위해서 사용
    - 연두색 박스(2번) : 물체가 존재하는 그리드 셀 i의 j번째 경계 박스
    - 파란색 박스(3번) : 해당 셀에 객체가 존재하지 않는 경우, 즉 배경인 경우에는 경계 박스 학습에 영향을 미치지 않도록 0.5의 가중치를 곱해주어서 패널티를 낮추기 위해 사용
    - 노란색 박스(4번) : 물체가 존재하지 않는 그리드 셀 i의 j번째 경계 박스( i번째 셀과 j번째 경계박스에 물체가 없는 경우에 수행 한다는 의미 )
    - 남색 박스(6번) : 물체가 있는 그리드 셀 i에서 바운딩 박스와는 상관 없이 클래스를 분류하기 위한 오차
    
- 최종 prediction에 포함된 바운딩 박스를 찾아내어 x, y 좌표, w, h 값, C 값의 예측 값과 ground truth 값의 차를 구해 모두 더해준다. 이 때, x, y, C 값은 그냥 단순 차를 구했고 w, h는 비율 값이기 때문에 루트를 씌워 차이를 구해준 점이 다르다.

- 1 noobj ij 라는 것은 물체가 없다고 판단된 그리드 셀 i의 j번째 바운딩 박스가 사실은 가장 ground truth와 IoU가 가장 높은 인덱스를 말한다. 즉, 물체로 찾아냈어야 하는데 못 찾아낸 인덱스입니다. 이에 대해선 찾아냈어야 하므로 C의 값의 차를 구해 loss에 더해준다.

- 마지막으로 모든 물체가 있다고 판단된 그리드 셀 i 들에 대해서 모든 클래스들에 대해서 예측 값과 실제 값의 차를 구해 더해준다.

**<loss 계산>**

**1 -> 그리드 셀 i의 j 경계박스에 물체가 있을 경우 경계 박스의 x, y 좌표의 loss 계산**

**2 -> 그리드 셀 i의 j 경계박스에 물체가 있을 경우 경계 박스의 width, height loss 계산**

**3 -> 그리드 셀 i의 j 경계박스에 물체가 있을 경우 클래스 score loss 계산**

**4 -> 그리드 셀 i의 j 경계박스에 물체가 없을 경우 클래스 score loss 계산**

**5 -> 물체가 있는 그리드 셀 i에 대해(경계박스는 상관 x) conditional class probability의 loss 계산**

# Performance # 

![image](https://user-images.githubusercontent.com/66320010/107518610-8e625a00-6bf2-11eb-84e8-a69608ac1079.png)

위 표를 보면 YOLO는 63.4 mAP를 가지며 초당 45장의 이미지를 처리 할 수 있음을 확인할 수 있다.

또한 YOLO가 가장 정확도가 높은 모델은 아니지만 이 논문이 작성될 당시에는 가장 빠른 처리속도를 가진 모델이고 영상에서도 프레임이 끊기지 않고 부드럽게 객체를 탐지할 수 있었다.

# 한계점 #

  - YOLO는 영상을 7x7 의 그리드셀로 분할하여 각 그리드 셀을 중심으로 하는 각종 크기의 오브젝트에 대해서 경계박스 후보를 2개 예측
  
    -> R-CNN 계열은 후보를 1천개 이상 제안하는것에 비해 YOLO는 총 7x7x2 = 98개의 후보를 제안하므로 이로 인해 성능이 떨어진다
    
    -> **그래서 한 오브젝트 주변에 여러개의 오브젝트가 있을 때 검출을 잘 못한다(ex. 새 떼처럼 작은 물체들이 모여있는 경우)**

# 참고 #

https://yeomko.tistory.com/19?category=888201

https://medium.com/curg/you-only-look-once-%EB%8B%A4-%EB%8B%A8%EC%A7%80-%ED%95%9C-%EB%B2%88%EB%A7%8C-%EB%B3%B4%EC%95%98%EC%9D%84-%EB%BF%90%EC%9D%B4%EB%9D%BC%EA%B5%AC-bddc8e6238e2

http://blog.naver.com/PostView.nhn?blogId=sogangori&logNo=220993971883






