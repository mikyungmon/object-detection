# YOLO v2 #

YOLO v2는 기존 YOLO 알고리즘의 두 번째 버전으로, 정확도를 높인 모델이며 YOLO v2 모델을 기반으로 무려 9000종류의 물체를 구분할 수 있는 YOLO 9000 모델을 공개하였다.

-> 이전까지 Object Detection 분야에서 가장 많이 사용되었던 데이터 셋인 coco가 약 80종류의 클래스를 가진 것과 비교하면 파격적

저자들은 해당 논문을 크게 **Better, Faster, Stronger**으로 나누었다.

# 1. Better #

Better 챕터에서 저자는 기존 YOLO 모델의 한계점으로 지적되었던 **정확도**를 어떻게 개선하였는지를 설명한다(어떤 방법들을 이용하여 recall과 localization성능을 높였는지)

**<성능 향상 요인>**

1) Batch normalization - 모든 컨볼루션 레이어에 배치 정규화를 추가

2) High Resolution Classifier - 높은 해상도 이미지로 backbone CNN 네트워크 fine tune

3) Anchor box - 경계 박스를 처음부터 직접 예측 -> 앵커 박스를 초기값으로 사용하여 예측

    3-1) Convolutional With Anchor Boxes - 기존 yolo에서 Fully Connected Layer를 떼어내고 Fully Convolutional Network 형태로 prediction을 계산

    3-2) Dimension Cluster - 실제 경계 박스들을 클러스터링하여 최적의 앵커박스를 찾음

    3-3) Direct Location Prediction - 경계 박스가 그리드 셀에서 벗어나지 않게 제약을 둠

4) Fine-Grained Features - 최종 feature map의 크기를 7x7에서 13x13으로 키움

5) Multi-Scale Training - 학습데이터의 크기를 320x320, 352x352, ..., 608x608 로 resize 하면서 다양한 스케일로 학습

## 1) Batch Normalization ##

YOLO v1에서 사용되고 있는 convolution layer에 모두 batch normalization을 적용시켜 mAP 2% 성능향상을 기록했다.

***batch normalization이란?***

![image](https://user-images.githubusercontent.com/66320010/107905330-b9053780-6f91-11eb-9454-3b39703ba86b.png)

*배치 정규화는 활성화함수의 활성화값 또는 출력값을 정규화(정규분포로 만든다)하는 작업을 말한다. 신경망의 각 layer에서 데이터(배치)의 분포를 정규화하는 작업이다. 일종의 노이즈를 추가하는 방법으로 (bias와 유사) 이는 배치마다 정규화를 함으로써 전체 데이터에 대한 평균의 분산과 값이 달라질 수 있다. 학습을 할 때마다 활성화값/출력값을 정규화하기 때문에 초기화(가중치 초깃값) 문제에서 비교적 자유로워진다.*

## 2) High Resolution Classifier ##

YOLO v1에서 feature extraction목적으로 사용된 CNN모델은 VGG 16기반이다.

VGG 16은 224 x 224 이미지로 resize되어 학습하기 때문에 224 x 224 이미지에 대한 객체를 classification하는데 최적화 되어있다.

즉, 기존 YOLO v1 모델은 224 x 224 크기의 해상도로 학습된 VGG 모델을 가져온 다음, 448 x 448 크기의 이미지에 대해서 Object Detection을 수행하게끔 구성되어 있어 해상도가 맞지 않았다. 

따라서 448 x 448 이미지에 익숙하지 않은 VGG 16 기반의 CNN모델은 detection에서 성능저하를 일으킨다.

**-> YOLO v2에서는 Object Detection 학습 전에 ImageNet의 데이터셋에서 448 x 448 이미지들을 다시 학습시켜 fine tuning해주었고 그 결과 약 4% mAP가 증가했다.**

![image](https://user-images.githubusercontent.com/66320010/107907254-80b42800-6f96-11eb-866d-0d2e129b4b84.png)

YOLO v2는 YOLO v1과 달리 VGG Net 기반의 모델을 사용하지 않고 자체적으로 만든 CNN모델인 "darknet 19"사용한다.

darknet 19는 앞서 말한 기존 CNN모델의 성능 저하 문제를 해결하기 위해 처음에 224 x 224에 대한 ImageNet 데이터셋을 학습시키고 난 후 448 x 448 이미지에 대해서 재학습 시켜주어 fine tuning해주는 것이다(darknet 19를 이용한 결과 4% mAP가 증가한 것이다)

## 3) Anchor boxes ##

YOLO v2에서는 기존에 사용했던 Fully Connected Layer를 떼어내고 Fully Convolutional Network 형태로 prediction을 계산하고, anchor box의 개념을 도입한다.

여기서 중요한 점은 5차원 박스를 예측할 때 (중심점 x,y 좌표, 박스 너비 높이 w, h, 물체일 확률 c) 이렇게 다섯가지 정보를 합친 벡터를 사용했다는 것입니다. 

이는 사전에 박스는 어떠한 형태일 것이다라는 사전 정보 없이 그냥 박스를 prediction 하는 것이다. 때문에 예측하는 박스의 크기나 위치가 중구난방이 될 우려가 있다. 

따라서 YOLO v2에서는 anchor box의 개념을 도입한다.

Faster R-CNN의 RPN 구조를 보면 convoultion연산을 통해 RoI를 추출하는 작업을 한다.

![image](https://user-images.githubusercontent.com/66320010/107908088-94608e00-6f98-11eb-9cf6-7eab470d5944.png)

YOLO v1은 x,y,w,h값을 랜덤하게 설정해주고 직접 최적화된 값을 찾아가도록 학습하지만 Faster R-CNN은 사전에 anchor box 9개를 설정해주고 그에 따른 x,y,aspect ratio, object confidence score를 계산하면 되는 구조여서 훨씬 간단히 학습할 수 있었다.

(아무것도 모르는 상태에서 정답을 찾아가는거 보다 어느정도 틀이 갖추어진 상태에서 정답을 찾아가는것이 더 쉽다고 생각하면 된다)

**YOLO v2에서는 Faster R-CNN과 비슷한 region proposal방식을 사용한다. anchor box를 미리 선정하고 사용하기 때문에 FC Layer를 사용하지 않고 RPN과 비슷하게 convolution layer를 사용하게 된다.**

**3-1) Convolutional With Anchor Boxes**

위에서 darknet 19를 학습시킬 때에는 448 x 448 이미지를 학습시킨다고 했는데 detection을 진행할 때는 입력 이미지가 416 x 416으로 바뀌게 된다.

그 이유는 7 x 7 grid cell은 크기가 너무 작기 때문에 사실상 low resolution 상태에서 detection하는 것과 같고, 이전 YOLO 모델에서 적용한 bounding box의 갯수를 보면 7 x 7 x2 = 98개인데 이는 recall(대상 물체를 빠뜨리지 않고 얼마나 잘 잡아내는지) 관점에서는 터무니 없이 적은 갯수이기 때문이다.

따라서 anchor box갯수를 2개에서 5개로 늘려 설정하고 7 x 7 보다 더 큰 feature map size인 13 x 13으로 설정하면 13 x 13 x 5 = 845개의 bounding box를 사용할 수 있기 떄문에 더 많은 recall을 얻을 수 있다(하지만 mAP성능에서는 오히려 더 낮은 결과가 나오게 되었다)

**-> YOLO v2에서는 보통 물체가 이미지의 중앙에 있는 경우가 많다보니 홀수 x 홀수로 최종 output feature map을 설정해주는 것이 더 좋기 때문에 14 x 14가 아닌 13 x 13으로 맞춰주기 위해서 448 x 448을 416 x 416으로 변경해준 것이다.**

**3-2) Dimension Clusters**

Faster R-CNN에서 사용한 anchor box는 사전에 aspect ratio와 size를 다르게 하여 9개의 숫자로 미리 정해주었다. 

하지만 YOLO v2에서는 anchor box를 적용시킬 때 단순히 aspect ratio와 size를 통해 미리 정해준 anchor box를 사용하는 것이 문제가 있다고 판단하였다.

![image](https://user-images.githubusercontent.com/66320010/107910999-eefce880-6f9e-11eb-9933-ae3dd2bd1275.png)

그래서 YOLO v2는 training dataset에 있는 ground truth bounding box에 k-means clustering방법을 사용하여 최적의 anchor boxes를 찾는다.

<k-means clustering 예시>

![image](https://user-images.githubusercontent.com/66320010/107911241-78acb600-6f9f-11eb-8276-d798dfd59306.png)

step 1. k를 3이라고 설정하면 임의의 데이터 3개를 지정하고 이 3개의 데이터를 기준으로 3개의 다른 영역을 구성하게 된다.

step 2. 3개의 다른 영역을 나눌 때, 지정된 3개의 data에서 가장 가까운 데이터들을 계산해 각각의 그룹을 형성한다.

step 3. 해당 그룹 내에서 데이터들간의 위치를 계산하여 그룹내에서 평균을 계산하고 그 평균을 기준으로 step2의 방식을 적용하여 다시 3개의 그룹을 형성한다.

step 4. step2와3을 반복하다보면 최적화된 clustering을 만들게 된다.

**최적의 anchor box를 찾을 때 euclidean clustering기준으로 k-means clustering방식을 적용하면 문제가 생긴다.**

![image](https://user-images.githubusercontent.com/66320010/107911385-d2ad7b80-6f9f-11eb-9c0b-dd553f8e31be.png)

위의 그림에서 파란색 박스가 ground truth bounding box, 빨간색 박스가 예측된 anchor box라고 할 때 두 bounding box는 비슷함에도 불구하고 중심점끼리 차이가 많이 나기 때문에 실제로 유사함에도 불구하고 무시되거나 같은 그룹이라고 보지 않을 확률이 많다.

또한 위 그림의 중간과 오른쪽과 같이 전혀 엉뚱한 groung truth와 anchor box가 그룹핑이 될 확률이 높아진다.

![image](https://user-images.githubusercontent.com/66320010/107911810-99c1d680-6fa0-11eb-8f87-ed252975fb6e.png)

**-> 따라서 논문에서는 IoU 개념을 이용해서 distance metric방식을 제안하였다(k-means clustering방식을 적용하여 최적의 anchor box를 추출할 때 euclidean distance가 아닌 IoU를 적용한다는 뜻이다)**

**3-3) Direct location prediction**

위의 과정을 통해 결정한 anchor box에 따라서 하나의 셀에서 5차원 벡터로 이루어진 bounding box를 예측한다. 

![image](https://user-images.githubusercontent.com/66320010/107911935-df7e9f00-6fa0-11eb-99b4-3c812a87d437.png)

   - cx, cy : 그리드 셀의 좌상단 끝 offset
   - pw, ph : prior anchor box의 width, height
   - tx, ty, tw, th : 우리가 예측한 박스의 x, y, w, h 값
   - bx, by, bw, bh : ground truth에 가까워지도록 계속해서 학습되는 trained anchor box의 x, y, w, h값

기존의 YOLO v1이 그리드 중심점을 예측했다면, YOLO v2는 그리드 셀의 좌측 상단 꼭지점으로부터 얼마나 이동하는지를 예측한다. 이것이 bx=σ(tx) + cx가 의미하는 바이다.

시그모이드 함수를 지나게 되면 tx,ty의 범위가 0~1로 바뀌기 때문에 predicted anchor box가 초기에 그려지는 범위가 해당 cell영역에 제한된다.

너비와 높이는 사전에 정의된 박스의 크기를 얼만큼 비율로 조절할 지를 지수승을 통해 예측하며, bw=pwe^tw에 나타나 있다.
   
**4) Fine-Grained Features**

상대적으로 작은 feature map에서는 작은 크기의 객체에 대한 localization작업이 힘들 수 있다. 그래서 feature map의 크기를 증가시켜 작은 크기의 객체 또한 잘 detection할 수 있도로 하기 위해 고안된 방법이 Fine-Grained Features이다.

기존의 YOLO v1과 달리 YOLO v2는 최종 output feature map이 13 x 13 크기를 갖고 있는데 이러한 output feature map보다 조금 더 큰 26 x 26 feature map위에서 bounding box작업을 진행한다.

![image](https://user-images.githubusercontent.com/66320010/107914092-0b9c1f00-6fa5-11eb-921a-b1beb5663a2e.png)

최종 feature map에서 적용한 13 x 13 x 512의 채널 수를 기준으로 26 x 26에서도 같은 채널 수를 유지하여 13 x 13 feature map에서 다루어 지는 정보들과 동일하게 설정해주되 26 x 26 에서는 4등분하여 적용시켜준다.

![image](https://user-images.githubusercontent.com/66320010/107914208-4e5df700-6fa5-11eb-841b-7c6ee024811f.png)

즉, 26 x 26 x 512의 feature map을 13 x 13 x (512 x 4)로 변환한 다음 detection을 위한 output으로 이어준다.

최종적으로 아래 그림과 같은 YOLO v2의 구조를 생각해 볼 수 있다.

![image](https://user-images.githubusercontent.com/66320010/107914309-8107ef80-6fa5-11eb-9868-ae972517b795.png)

*YOLO v2에서는 상위 레이어의 피쳐맵을 하위 피쳐맵에 합쳐주는 passthrough layer를 도입한 것이다. 위 그림에서 볼 수 있듯이 높은 해상도를 가진 26 x 26 x 256 피쳐맵을 13 x 13 x 2048 크기로 리스케일하여 낮은 해상도의 피쳐맵과 합쳐 13 x 13 x 3072 크기의 피쳐맵을 만들어 낸 것이다.*

**5) Multi-Scale Training**

작은 물체들을 잘 잡아내기 위해서 YOLO v2는 여러 스케일의 이미지를 학습할 수 있도록 하였다.

Fully Connected Layer를 떼어냈기 때문에 입력 이미지의 해상도에서 비교적 자유로울 수 있게 되었고 YOLO v2는 이를 활용하여 학습 시에 {320, 352, ..., 608} 와 같이 32 픽셀 간격으로 매 10 배치시마다 입력 이미지의 해상도를 바꿔주며 학습을 진행하였다.

![image](https://user-images.githubusercontent.com/66320010/107915422-cd542f00-6fa7-11eb-8a9e-6881feff614f.png)

그 결과 기존의 YOLO v1보다 정확도가 높은 모델을 만들어 냈다.

# 2. Faster #

Faster 챕터에서 저자는 YOLO v2가 YOLO v1보다 속도 측면에서 어떤 개선을 이루었는지 설명한다.

핵심은 YOLO v1이 pretrained VGG 혹은 Googlenet을 사용하였는데 이 백 본 네트워크가 너무 크고 복잡하기 때문에 새로운 CNN 아키텍처인 Darknet을 사용한다는 것이다(구조는 위의 사진 참고)

전체 구조는 VGG와 크게 다르지 않지만 MaxPooling을 줄이고 컨볼루션 연산을 늘렸다.

또한 마지막에 Fully Connected Layer를 제거하고 Convolution 연산으로 대체하여 파라미터의 수를 줄인 것을 볼 수 있다. 

YOLO v2는 이러한 경량 CNN 아키텍쳐를 사용하여 속도 측면에서도 개선을 이루었다.

# 3. Stronger #

YOLO v1은 pascal VOC 데이터셋에서 제공되는 20개의 클래스를 기준으로 학습하기 때문에 20개의 클래스에 대해서 밖에 detection하지 못한다. 

하지만 YOLO v2에서는 9000개의 클래스에 대해서 detection할 수 있게 되었다.

![image](https://user-images.githubusercontent.com/66320010/107916370-9121ce00-6fa9-11eb-936c-8e54e0baf300.png)

   **[Hierarchical classification]**

   저자는 class에 대해서 classifiacation을 수행할 경우 우선 계층적으로 분류 작업을 수행해야 한다고 제시한다. 

   이미지 넷 데이터를 보면 개과 안에 웰시코기, 요크셔테리어와 같은 라벨들이 속한다. 

   이 점에 착안하여 저자는 소프트맥스 연산을 수행할 때, 전체 클래스에 대해서 한번에 수행하는 것이 아니라 각 대분류 별로 소프트맥스를 수행할 것을 제안한다.

   **[Dataset combination with WordTree]**

![image](https://user-images.githubusercontent.com/66320010/107916914-6e43e980-6faa-11eb-8b9b-46fa53de4116.png)

   위 사진과 같은 계층 구조를 도입하여 저자는 coco와 imagenet 데이터 셋의 라벨을 트리 구조를 활용하여 섞는다.

   **[Joint classification and detection]**
   
   앞서 wordtree를 이용하여 총 9418개의 클래스를 가진 데이터 셋을 만들어 냈다(ImageNet Classification + COCO) 
   
   그러나 이 중 9000개의 클래스는 ImageNet Classification 데이터 셋에 속하였고, 이들은 Classification 라벨만 붙어있는 상태이다.
   
   저자는 학습 과정에서 COCO 데이터 셋이 더 많이 샘플링 되도록 하여 실제 모델이 학습하는 이미지의 비율을 4:1로 맞춰주었다. 
   
   그리고 Classification 라벨만 붙어있는 이미지의 경우에는 Classification Loss만 역전파가 되게끔 하였다. 
   
   이를 통해서 Classification과 Object Detection 테스크가 섞여있는 데이터 셋을 학습시킬 수 있게 되었다.

# Result #

![image](https://user-images.githubusercontent.com/66320010/107917898-08586180-6fac-11eb-9b48-d17b6b8f1974.png)

저자는 학습시킨 YOLO 9000을 ImageNet Detection Challenge 데이터 셋을 활용하여 성능 평가를 진행하였고, 19.7 mAP를 얻었다. 

특히 detection 라벨이 붙은 데이터를 하나도 학습하지 못한 156개의 클래스에 대해서는 16.0 mAP라는 정확도를 달성한다. 

수치만 놓고 보면 실망스러울 수 있지만, 모델이 무려 9000개의 클래스를 구분하도록 학습을 진행다는 것을 생각해보면 놀라운 결과라고 할 수 있다.

***

# YOLO v3 # 

![image](https://user-images.githubusercontent.com/66320010/107918392-ed3a2180-6fac-11eb-8180-db16c6a6c595.png)

YOLO v3는 YOLO 모델에 당시에 등장한 기법들을 적용하여 성능을 향상시킨 모델이다.

YOLO v3의 성능이 그래프의 축을 뚫고 나감으로써 성능이 훨씬 뛰어나다는 것을 보여준다.

 - **앞선 모델들과의 차이점**
    
    - feature extractor
    
      Darknet-19 -> Darknet-53으로 변경
    
    - Predictions Accross Scales
    
      3개의 bounding box, 3개의 feature map 활용 (다른 scale, 각각 2배씩 차이) => 총 9개(3개 바운딩박스 x 3개 피쳐맵)의 anchor box는 k-means clustering을 통해 결정
     
      -> (10x13), (16x30), (33x23), (30x61), (62x45), (59x119), (116x90), (156x198), (373x326)
    
    -  Class Prediction
    
    multi-label이 있을 수 있으므로 class prediction으로 softmax를 쓰지 않고 independent logistic classifiers를 사용
    
    따라서 loss term도 binary cross-entropy로 변경 -> 이는 좀 더 복잡한 데이터셋(Open Image Dataset)을 학습하는데 도움
    
 - **Darknet 53**
 
 ![image](https://user-images.githubusercontent.com/66320010/107919306-6e45e880-6fae-11eb-8966-83b10033fd65.png)

 이전 YOLO v2에서는 VGG 모델이 지나치게 복잡하다는 점에 착안하여 훨씬 적은 파라미터를 사용하면서도 성능이 좋은 Darknet-19 아키텍쳐를 사용했다.
 
 Darknet-53에서는 Darknet-19에 ResNet에서 제안된 skip connection 개념을 적용하여 레이어를 훨씬 더 많이 쌓은 모습을 보여준다.
 
 아키텍쳐를 살펴보면 먼저 3 x 3 컨볼루션과 1 x 1 컨볼루션으로 이루어진 블럭을 연속해서 쌓아가며 MaxPooling 대신에 컨볼루션의 stride를 2로 취해주어 feature map의 해상도를 줄여나간다.
 
 또한 skip connection을 활용해서 residual 값을 전달하고 마지막 레이어에서 Average Pooling과 Fully Connected Layer를 통과한 뒤, Softmax를 거쳐 분류 결과를 출력한다.
 
 ![image](https://user-images.githubusercontent.com/66320010/107920330-2031e480-6fb0-11eb-8c2f-2b9a81bdd8a8.png)
 
 아키텍쳐만 살펴보아서는 기존의 ResNet과 큰 차이점이 없어 보이지만 저자들은 실험 결과 ResNet-101과 ResNet-152 과 정확도는 큰 차이가 안나지만 FPS가 훨씬 높다는 점을 강조한다.
   
 - **structure**

![image](https://user-images.githubusercontent.com/66320010/107920463-5b341800-6fb0-11eb-8369-9eab2ca8636a.png)

# 참고 # 

https://yeomko.tistory.com/47

https://yeomko.tistory.com/48

https://89douner.tistory.com/93

https://taeu.github.io/paper/deeplearning-paper-yolov3/

