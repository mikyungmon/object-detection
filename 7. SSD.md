# SSD(Single Shot MultiBox Detector) #

![image](https://user-images.githubusercontent.com/66320010/108501327-7aa2ac00-72f4-11eb-97e7-18038c3d6410.png)

  - YOLO v1은 속도 측면에서 당시 Faster R-CNN이 7FPS이었던 것을 45FPS까지 끌어올리는 비약적인 발전을 이루었다.
  
  - 하지만 **정확도** 측면에서는 한계점이 있었고 **새 떼처럼 작은 물체들이 모여있는 경우에는 잘 잡아내지 못했다.**
  
    **=> SSD는 바로 이러한 한계점을 극복하고자 하는 시도에서 출발하였다.**
    
  - SSD의 핵심 아이디어
  
    - YOLO v1의 문제점은 입력 이미지를 7x7 의 그리드 셀로 분할하여 각 그리드 셀을 중심으로 하는 각종 크기의 오브젝트에 대해서 경계박스 후보를 2개 예측하여 기존 R-CNN 계열은 후보를 1천개 이상 제안하는것에 비해 YOLO v1은 총 7x7x2 = 98개의 후보를 제안하므로 이로 인해 성능이 떨어진다는 것이다.
    
    - 또한 신경망을 모두 통과하면서 컨볼루션과 풀링을 거쳐 coarse한 정보만 남은 마지막 단 feature map만 사용하기 때문에 정확도가 하락하는 한계가 있었다.
    
    - 따라서 SSD는 이전 연구들에서 장점만 모아 이러한 YOLO v1의 한계점을 극복하고자 하였다.
  
    **"fully convolution network에서처럼 앞단 컨볼루션 feature map을 끌어와 사용하여 디테일을 잡아내고 Faster RCNN의 anchor 개념을 가져와 다양한 형태의 object들도 잡아낼 수 있도록 한다."**
    
# Architecture # 
  
![image](https://user-images.githubusercontent.com/66320010/108503394-a8d5bb00-72f7-11eb-9563-3092550e066b.png)

  - SSD는 YOLO v1과 달리 컨볼루션 과정을 거치는 중간 중간 feature map들에서 모두 Object Detection을 수행한다.
  
  - SSD는 300 x 300 크기의 이미지를 입력받아서 ImageNet으로 pretrained된 VGG의 Conv5_3층까지 통과하며 feature를 추출한다.

  - 그 다음, 추출된 feature map을 컨볼루션을 거쳐 다음 층으로 넘겨주는 동시에 object detection을 수행한다.

  **-> 이전 Fully Convolution Network에서 컨볼루션을 거치면서 디테일한 정보들이 사라지는 문제점을 앞단의 feature map들을 끌어오는 방식으로 해결한 것이다.**
  
 이것을 좀 더 자세하게 표현한 그림은 아래와 같다.
 
 ![image](https://user-images.githubusercontent.com/66320010/108531969-a6d12380-731a-11eb-91ca-af68d369288b.png)
 
VGG를 통과하여 얻은 feature map을 대상으로 쭉쭉 컨볼루션 진행하여 최종적으로는 1 x 1크기의 feature map을 뽑는다.

그리고 각 단계별로 추출된 feature map은 Detector & Classifier를 통과하여 Object Detection을 수행한다.

![image](https://user-images.githubusercontent.com/66320010/108532467-3971c280-731b-11eb-9c55-9e12b8b434c8.png)

위의 그림은 Detector & Classifier의 구조이다.

예를 들어 컨볼루션 중간에 5  x 5 x 256 크기의 피쳐맵을 대상으로 Object Detection을 수행한다고 가정해보자(여기서 5 x 5는 YOLO에서 그리드 크기에 해당한다고 생각하면 된다)

![image](https://user-images.githubusercontent.com/66320010/108732639-193b4100-7571-11eb-949a-18784cde82ac.png)

Detector & Classifier를 통해서 **경계 박스 정보**와 **클래스 정보**를 얻는다.

우선 Detector & Classifier의 구조를 보면 feature map에 3 x 3 컨볼루션을 적용해 bounding box regression값을 계산한다.

이는 각각 default box들의 x, y ,w, h의 조절값을 나타내므로 4차원 벡터에 해당하며 위 그림에서는 그리드 셀 하나에 3개의 default box를 적용하였기 때문에 결과 feature map의 크기는 5 x 5 x 12가 된다.

마지막으로 각각의 default box마다 모든 클래스에 대하여 classification을 진행하는데 총 20개의 클래스 + 1(배경 클래스) x default box 수 이므로 최종 feature map의 크기는 5 x 5 x 63이 된다.

원래는 conv9_2로부터 5 x 5 x (6 x (Classes + 4)) 의 피처맵이 만들어지는데 위 그림은 5 x 5 x (3 x (Classes + 4)) 로 채널수가 좀 다르다(6개 다 그리기 힘들어서 3개만 표현한 거 같다)

SSD는 하나의 그리드마다 크기가 각기 다른 default box(비율과 크기가 각기 다른 기본박스를 설정해놓은 것)들을 먼저 계산하는데 그림으로 나타내면 아래와 같다.

(각각의 feature map을 가져와 비율과 크기가 각기 다른 dafault box를 투영하고 이렇게 찾아낸 박스들에 bounding box regression을 적용하고 confidence level을 계산한다) **-> 이는 YOLO가 아무런 기본 값 없이 2개의 박스를 예측하도록 한 것과 대조적이다.**

![image](https://user-images.githubusercontent.com/66320010/108734806-2ce7a700-7573-11eb-9f7a-bf2363a4d2d1.png)

VGG16 conv4_3에 이어서 Layer를 거칠수록 점점 Feature Map 크기가 감소하고 이 특성을 이용하여 다양한 배율에서의 Object Detection이 가능하다는 것을 보여주는 그림이다. 

즉, 위 사진과 같이 8 × 8 Feature Map에서는 작은 물체인 고양이를 찾고 4 × 4 Feature Map에서는 큰 물체인 개를 찾을 수 있다는 것이다.

**높은 해상도의 feature map에서는 작은 물체를 잘 잡아낼 수 있고 낮은 해상도에서는 큰 물체를 잘 잡아낸 것으로 추측할 수 있다.** 

**Default Boxes Generating**

  - default box를 어떻게 구하는지에 대해 알아볼 것이다.
  
  - 먼저 Object Detection을 수행할 Feature Map의 개수를 m으로 놓고, Feature Map의 인덱스(그리드)를 k로 놓는다. 

  - 각 피쳐맵 별로 Scale Level은 아래 수식으로 구한다.

  ![image](https://user-images.githubusercontent.com/66320010/108541721-244e6100-7326-11eb-93cf-0ace6622519f.png)
  
  - s_min은 0.2, s_max는 0.9로 설정하며 위의 수식은 큰 의미 없고 min과 max를 잡은 다음 그 사이를  m값에 따라서 적당히 구간을 나누어주는 값이다. m=6으로 설정했을 때의 결과는 [0.2, 0.34, 0.48, 0.62, 0.76, 0.9]가 된다.

  -> 이 값은 각의 피쳐맵에서 default box의 크기를 계산할 때 입력 이미지의 너비, 높이에 대해서 얼만큼 큰 지를 나타내는 값이다.
  
  **->즉, 첫 번째 피쳐맵에선 입력 이미지 크기의 0.2 비율을 가진 작은 박스를 default box로 놓겠다는 의미이며 마지막 피쳐맵에서는 0.9와 같이 큰 default box를 잡겠다는 의미이다.** (생략할지말지?)

**(위의 내용을 정리 및 보충)**

1) Modified VGG Net

   먼저 SSD는 VGG Net을 이용해 feature extraction하려고 했는데 이것을 그대로 이용하지 않고 SSD 모델에 맞게 수정해주었다. 아래 그림에서 어떻게 수정되었는지 설명하고 있다.

   ![image](https://user-images.githubusercontent.com/66320010/108504036-a45dd200-72f8-11eb-9982-7abda4f1b4fd.png)
   
2) Remove FC layer

   앞서 YOLO v2에서 FC layer를 제거시켜서 1) object detection 모델에서 입력 이미지를 고정시키지 않아도 된다. 2) parameters 갯수의 급격한 감소로 속도가 빨라진다. 는 효과를 얻었다.
   
   본래 YOLO v2는 SSD 다음에 나온 모델이기 때문에 SSD는 YOLO v2에서 FC layer를 제거하는 아이디어를 얻었을 가능성이 크다.
   
 3) Multi-map scale feature maps

    위에서 SSD 모델의 구조를 보면 detection을 하기위해 여러개의 feature map을 사용하는 것을 볼 수 있다.
   
    SSD 역시 그리드 셀 방식으로 운영이 되는데 YOLO v1에서 처럼 하나의 그리드 셀에서 고정된 갯수의 default 앵커 박스를 사용하게 된다.
   
   ![image](https://user-images.githubusercontent.com/66320010/108505350-96a94c00-72fa-11eb-890d-8f69fefa46bf.png)
   
   예를 들어 하나의 그리드 셀에서 생성하는 앵커 박스들의 크기가 동일하다면 작은 output feature map에서는 좀 더 큰 객체를 더 잘 탐지할 수 있고, 더 큰 output feature map에서는 작은 물체를 검출할 수 있는 가능성이 커진다. 
   
   그래서 **다양한 크기의 feature map**이 detection에서 좀 더 좋은 성능을 기대해 볼 수 있게 되는 것이다.
   
 4) Detection

    앞서 SSD의 detection방식은 다양한 크기의 output feature map을 사용하는 것이었다.
   
    다양한 output feature map이 최종단계에서 어떻게 결합되는지 아래 그림을 통해 이해할 수 있다.
   
    ![image](https://user-images.githubusercontent.com/66320010/108536867-493fd580-7320-11eb-9ddd-86f12703e417.png)
   
    그림을 보면 각각의 컨볼루션에서 앵커 박스의 갯수를 다르게 설정했음을 알 수 있는데 그 이유는 명확하지 않으나 추측하기로는 아마 몇몇의 feature map에서 empirical하게 박스 갯수들에 대해서 실험을 해봤던 거 같다.
   
    그래서 몇몇의 feature map에서는 다른 default box shapes를 사용했다고 하는데 앵커박스 갯수가 6개인 곳은 기존 4개와 2개의 다른 앵커박스 모양이 추가된다.
    
    ![image](https://user-images.githubusercontent.com/66320010/108538049-b30caf00-7321-11eb-9534-bfc9778e6065.png)
    
 5) NMS

    위에서 보았듯이 8742 x (classes + 4) features를 뽑게되면 각각의 output feature map에서 뽑은 앵커박스들 때문에 최종 단계에서 한 객체에 대한 많은 bounding box가 생성된다.
    
    ![image](https://user-images.githubusercontent.com/66320010/108539040-ce2bee80-7322-11eb-9e61-00bb4fb7bef9.png)
    
    따라서 이전에 YOLO 에서 설명한 NMS 알고리즘에 따라 하나의 앵커 박스만 선택한다.
    
# Loss Function #

![image](https://user-images.githubusercontent.com/66320010/108542793-88255980-7327-11eb-8821-8c266191a1bc.png)

  - 전체적으로 Loss는 YOLO의 Loss와 매우 유사하다. 

  - 전체 loss = 각 클래스 별로 예측한 값과 실제 값 사이의 차(Lconf) + bounding box regression 예측값과 실제 값 사이의 차(Lloc)

  **[Lconf]**
   
  ![image](https://user-images.githubusercontent.com/66320010/108543107-f8cc7600-7327-11eb-9cae-0b94902f5575.png)
   
   - 𝑥_𝑖𝑗^𝑝 는 특정 그리드의 i번째 default box가 p클래스의 j번째 ground truth box와 매치된다는 의미이다.

   - 즉, 모델이 물체가 있다고 판별한 default box들 가운데서 해당 박스의 groud truth박스하고만 cross entropy loss를 구한다는 의미이다.


  **[Lloc]**
  
  ![image](https://user-images.githubusercontent.com/66320010/108543806-e9016180-7328-11eb-9c5f-a979e4710972.png)
  
  - smoothL1은 앞서 Fast RCNN에서 제시된 bounding box regression loss이다.

  - 그 아래는 bounding box regression 시에 사용하는 예측 값들을 말한다. 

  - x, y 좌표 값은 절대 값이기 때문에 예측값과 실제 값 사이의 차를 default 박스의 너비 혹은 높이로 나누었다 -> 이렇게 해주면 값이 0에서 1 사이로 정규화되는 효과가 있다


# 참고 #

https://yeomko.tistory.com/20

https://89douner.tistory.com/94

https://taeu.github.io/paper/deeplearning-paper-ssd/

















